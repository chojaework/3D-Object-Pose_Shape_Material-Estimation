{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from utils.visualize import *\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import open3d as o3d\n",
    "import transforms3d\n",
    "from utils.preprocess import path_dict\n",
    "from utils.read_json import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scene0015_00.9.03001627.e6328c1bb6b194f262e682c9809bff14.npy', 'scene0015_00.8.03001627.e6328c1bb6b194f262e682c9809bff14.npy', 'scene0015_00.7.03001627.e6328c1bb6b194f262e682c9809bff14.npy', 'scene0015_00.15.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.25.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.19.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.18.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.17.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.1.03001627.e6328c1bb6b194f262e682c9809bff14.npy', 'scene0015_00.14.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.0.03001627.73f9aa75944ecf0b9debdd405104de8c.npy', 'scene0015_00.11.03001627.e6328c1bb6b194f262e682c9809bff14.npy', 'scene0015_00.10.03001627.e6328c1bb6b194f262e682c9809bff14.npy', 'scene0015_00.24.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.20.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.16.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.23.03001627.1be38f2624022098f71e06115e9c3b3e.npy']\n",
      "['scene0015_00.6.04379243.fb50672ad3f7b196cae684aee7caa8d9.npy', 'scene0015_00.2.04379243.fb50672ad3f7b196cae684aee7caa8d9.npy', 'scene0015_00.13.04379243.899f815eaad62b56d8cad143689f8b51.npy', 'scene0015_00.5.04379243.fb50672ad3f7b196cae684aee7caa8d9.npy', 'scene0015_00.3.04379243.fb50672ad3f7b196cae684aee7caa8d9.npy', 'scene0015_00.4.04379243.fb50672ad3f7b196cae684aee7caa8d9.npy']\n"
     ]
    }
   ],
   "source": [
    "scans = os.listdir(\"/scannet/crop_scan2cad_filter/data\")\n",
    "\n",
    "chairs = []\n",
    "tables = []\n",
    "for scan in scans:\n",
    "    if scan[:12] == \"scene0015_00\" and scan.split('.')[2] == \"03001627\":\n",
    "        chairs.append(scan)\n",
    "    if scan[:12] == \"scene0015_00\" and scan.split('.')[2] == \"04379243\":\n",
    "        tables.append(scan)\n",
    "        \n",
    "print(chairs)\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scene0015_00.9.03001627.e6328c1bb6b194f262e682c9809bff14.npy', 'scene0015_00.8.03001627.e6328c1bb6b194f262e682c9809bff14.npy', 'scene0015_00.7.03001627.e6328c1bb6b194f262e682c9809bff14.npy', 'scene0015_00.15.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.25.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.19.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.21.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.22.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.18.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.17.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.1.03001627.e6328c1bb6b194f262e682c9809bff14.npy', 'scene0015_00.14.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.0.03001627.73f9aa75944ecf0b9debdd405104de8c.npy', 'scene0015_00.11.03001627.e6328c1bb6b194f262e682c9809bff14.npy', 'scene0015_00.10.03001627.e6328c1bb6b194f262e682c9809bff14.npy', 'scene0015_00.24.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.20.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.16.03001627.1be38f2624022098f71e06115e9c3b3e.npy', 'scene0015_00.23.03001627.1be38f2624022098f71e06115e9c3b3e.npy']\n",
      "['scene0015_00.6.04379243.fb50672ad3f7b196cae684aee7caa8d9.npy', 'scene0015_00.2.04379243.fb50672ad3f7b196cae684aee7caa8d9.npy', 'scene0015_00.13.04379243.899f815eaad62b56d8cad143689f8b51.npy', 'scene0015_00.12.04379243.899f815eaad62b56d8cad143689f8b51.npy', 'scene0015_00.5.04379243.fb50672ad3f7b196cae684aee7caa8d9.npy', 'scene0015_00.3.04379243.fb50672ad3f7b196cae684aee7caa8d9.npy', 'scene0015_00.4.04379243.fb50672ad3f7b196cae684aee7caa8d9.npy']\n"
     ]
    }
   ],
   "source": [
    "scans = os.listdir(\"/scannet/crop_scan2cad\")\n",
    "\n",
    "chairs = []\n",
    "tables = []\n",
    "for scan in scans:\n",
    "    if scan[:12] == \"scene0015_00\" and scan.split('.')[2] == \"03001627\":\n",
    "        chairs.append(scan)\n",
    "    if scan[:12] == \"scene0015_00\" and scan.split('.')[2] == \"04379243\":\n",
    "        tables.append(scan)\n",
    "        \n",
    "print(chairs)\n",
    "print(tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import argparse\n",
    "from trainer.RetrievalTrainer import trainer\n",
    "from datasets.ScannetDataset import ScannetDataset\n",
    "from datasets.CategoryTestTimeDataset import *\n",
    "from datasets.Reader import *\n",
    "\n",
    "from test_time.FeatureExtractor import FeatureExtractor\n",
    "from test_time.RetrievalModule import RetrievalModule\n",
    "\n",
    "from utils.logger import logger\n",
    "from utils.ckpts import load_checkpoint, save_checkpoint\n",
    "from utils.retrieval import *\n",
    "from utils.visualize import *\n",
    "from utils.preprocess import random_rotation, load_norm_pc, apply_transform\n",
    "from utils.symmetry import*\n",
    "from utils.eval_pose import *\n",
    "from utils.Info.Scan2cadInfo import Scan2cadInfo\n",
    "from utils.Info.CADLib import *\n",
    "\n",
    "from model import load_model, fc\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import MinkowskiEngine as ME\n",
    "\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "\n",
    "class Config():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.root = \"/scannet/ShapeNetCore.v2.PC15k\"\n",
    "        self.scan2cad_root = \"/scannet/crop_scan2cad_filter/data\"\n",
    "        self.cad_root = \"/scannet/ShapeNetCore.v2.PC15k\"\n",
    "        #self.catid = \"04379243\"\n",
    "        self.catid = \"03001627\"\n",
    "        self.voxel_size = 0.03\n",
    "        self.dim = [1024, 512,  256]\n",
    "        self.embedding = \"conv1_max_embedding\"\n",
    "        #self.resume = \"./ckpts/scannet_table_ret_max_sim0\"\n",
    "        self.resume = \"./ckpts/scannet_ret_max_sim0\"\n",
    "        self.model = \"ResUNetBN2C\"\n",
    "        self.model_n_out = 16\n",
    "        self.normalize_feature = True\n",
    "        self.conv1_kernel_size = 3\n",
    "        self.bn_momentum = 0.05\n",
    "        self.nn_max_n = 500\n",
    "        self.lib_path = \"./CadLib\"\n",
    "        self.scan2cad_dict = \"/scannet/scan2cad_download_link/unique_cads.csv\"\n",
    "        self.annotation_dir = \"/scannet/scan2cad_download_link\"\n",
    "\n",
    "\n",
    "config_chair = Config()\n",
    "config_chair.catid = \"03001627\"\n",
    "config_chair.resume = \"./ckpts/scannet_ret_max_sim0\"\n",
    "config_table = Config()\n",
    "config_table.catid = \"04379243\"\n",
    "config_table.resume = \"./ckpts/scannet_table_pose_001_usedLib_bestMatch_Best\"#\"./ckpts/scannet_table_ret_max_sim0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chamfer_gpu_1direction(pc0, pc1):\n",
    "\n",
    "    pc0 = pc0[None,:,:]\n",
    "    pc1 = pc1[:,None,:]\n",
    "    delta = pc0 - pc1\n",
    "    return delta.norm(dim=2).min(0)[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Extraction\n",
      "Embedding: conv1_max_embedding\n",
      "FC dim: [1024, 512, 256]\n",
      "voxel size: 0.03\n",
      "loading checkpoint from ./ckpts/scannet_ret_max_sim0\n",
      "Model at epoch: 99\n",
      "Feature Extraction\n",
      "Embedding: conv1_max_embedding\n",
      "FC dim: [1024, 512, 256]\n",
      "voxel size: 0.03\n",
      "loading checkpoint from ./ckpts/scannet_table_pose_001_usedLib_bestMatch_Best\n",
      "Model at epoch: 94\n"
     ]
    }
   ],
   "source": [
    "feature_extractor_chair = FeatureExtractor(config_chair)\n",
    "feature_extractor_table = FeatureExtractor(config_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 652/652 [00:01<00:00, 633.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval feature Index: 1/21\n",
      "Eval feature Index: 11/21\n",
      "Eval feature Index: 21/21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 830/830 [00:01<00:00, 685.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish Loading\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval feature Index: 1/26\n",
      "Eval feature Index: 11/26\n",
      "Eval feature Index: 21/26\n"
     ]
    }
   ],
   "source": [
    "retrieval_module_chair = RetrievalModule(config_chair, feature_extractor_chair, \"scan2cad\", update=True)\n",
    "retrieval_module_table = RetrievalModule(config_table, feature_extractor_table, \"scan2cad\", update=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['scene0653_00.9.03001627.60647c98b5fc96b13002761e7a3ba3bd.npy', 'scene0653_00.10.03001627.60647c98b5fc96b13002761e7a3ba3bd.npy', 'scene0653_00.11.03001627.60647c98b5fc96b13002761e7a3ba3bd.npy', 'scene0653_00.7.03001627.60647c98b5fc96b13002761e7a3ba3bd.npy', 'scene0653_00.5.03001627.60647c98b5fc96b13002761e7a3ba3bd.npy', 'scene0653_00.8.03001627.60647c98b5fc96b13002761e7a3ba3bd.npy', 'scene0653_00.21.03001627.7212bdfab35f72b23002761e7a3ba3bd.npy', 'scene0653_00.12.03001627.60647c98b5fc96b13002761e7a3ba3bd.npy']\n",
      "['scene0653_00.15.04379243.24942a3b98d1bcb6a570c6c691c987a8.npy', 'scene0653_00.14.04379243.24942a3b98d1bcb6a570c6c691c987a8.npy', 'scene0653_00.13.04379243.24942a3b98d1bcb6a570c6c691c987a8.npy', 'scene0653_00.16.04379243.24942a3b98d1bcb6a570c6c691c987a8.npy', 'scene0653_00.25.04379243.24942a3b98d1bcb6a570c6c691c987a8.npy', 'scene0653_00.17.04379243.24942a3b98d1bcb6a570c6c691c987a8.npy']\n"
     ]
    }
   ],
   "source": [
    "root = \"/scannet/crop_scan2cad\"\n",
    "scene_id = \"scene0653_00\"\n",
    "scans = os.listdir(root)\n",
    "\n",
    "chairs = []\n",
    "tables = []\n",
    "for scan in scans:\n",
    "    if scan[:12] == scene_id and scan.split('.')[2] == \"03001627\":\n",
    "        chairs.append(scan)\n",
    "    if scan[:12] == scene_id and scan.split('.')[2] == \"04379243\":\n",
    "        tables.append(scan)\n",
    "        \n",
    "print(chairs)\n",
    "print(tables)\n",
    "\n",
    "save_dir = os.path.join(\"/zty-vol/results/scene_reconstruct\", scene_id)\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scannet/ShapeNetCore.v2.PC15k/03001627/train/6b64af50e21c0006f91663a74ccd2338.npy']\n",
      "['/scannet/ShapeNetCore.v2.PC15k/03001627/train/6f36520144753550f91663a74ccd2338.npy']\n",
      "['/scannet/ShapeNetCore.v2.PC15k/03001627/train/7212bdfab35f72b23002761e7a3ba3bd.npy']\n",
      "['/scannet/ShapeNetCore.v2.PC15k/03001627/train/6fde09bdd613f6e6492d9da2668ec34c.npy']\n",
      "use ransac\n",
      "['/scannet/ShapeNetCore.v2.PC15k/03001627/train/f4cec47ced59d95a3002761e7a3ba3bd.npy']\n",
      "use ransac\n",
      "['/scannet/ShapeNetCore.v2.PC15k/03001627/train/6b64af50e21c0006f91663a74ccd2338.npy']\n",
      "use ransac\n",
      "['/scannet/ShapeNetCore.v2.PC15k/03001627/train/8a948db5f12d02af492d9da2668ec34c.npy']\n",
      "['/scannet/ShapeNetCore.v2.PC15k/03001627/train/6b64af50e21c0006f91663a74ccd2338.npy']\n",
      "use ransac\n",
      "['/scannet/ShapeNetCore.v2.PC15k/04379243/test/4bbf789edb243cafc955e5ed03ef3a2f.npy']\n",
      "['/scannet/ShapeNetCore.v2.PC15k/04379243/train/24942a3b98d1bcb6a570c6c691c987a8.npy']\n",
      "['/scannet/ShapeNetCore.v2.PC15k/04379243/train/968714b674baa3152cf0938654a53e55.npy']\n",
      "['/scannet/ShapeNetCore.v2.PC15k/04379243/test/c6c412c771ab0ae015a34fa27bdf3d03.npy']\n",
      "['/scannet/ShapeNetCore.v2.PC15k/04379243/train/24942a3b98d1bcb6a570c6c691c987a8.npy']\n",
      "['/scannet/ShapeNetCore.v2.PC15k/04379243/train/ea9e7db452d2df55d42ec7e303174a87.npy']\n"
     ]
    }
   ],
   "source": [
    "visual_pcs = []\n",
    "colors = []\n",
    "\n",
    "for idx, name in enumerate(chairs):\n",
    "    pc = np.load(os.path.join(root, name))\n",
    "    offset = pc.mean(0)\n",
    "    pc_center = pc - offset\n",
    "    r = np.linalg.norm(pc_center, 2, 1).max()\n",
    "    pc_center /= r\n",
    "    #Jvisualize([pc_center], [\"RED\"])\n",
    "    \n",
    "    #feature extraction\n",
    "    base_local_feat, base_global_feat, base_coords = feature_extractor_chair.process(pc_center)\n",
    "\n",
    "    # retrieval\n",
    "    ret_path = retrieval_module_chair.TopN(base_global_feat.detach().cpu().numpy())\n",
    "    print(ret_path)\n",
    "    ret_raw_pc = load_raw_pc(ret_path[0], 10000)\n",
    "    \n",
    "    norm_pc_offset = ret_raw_pc.mean(0)\n",
    "    ret_pc = ret_raw_pc - norm_pc_offset\n",
    "    norm_pc_r = np.max(np.linalg.norm(ret_pc,2,1))\n",
    "    ret_pc = ret_pc/norm_pc_r\n",
    "    \n",
    "    ret_local_feat, ret_global_feat, ret_coords = feature_extractor_chair.process(ret_pc)\n",
    "    \n",
    "    #pose estimation\n",
    "    k_nn=5\n",
    "    max_corr = 0.20\n",
    "\n",
    "    baseF, xyz0, posF, xyz1 = ret_local_feat, ret_coords, base_local_feat, base_coords\n",
    "\n",
    "    \"\"\"\n",
    "    idx_0, idx_1 = find_kcorr(baseF, posF, k=k_nn, subsample_size=-1)\n",
    "\n",
    "    source_pcd = xyz0[idx_0]\n",
    "    target_pcd = xyz1[idx_1]\n",
    "\n",
    "    T_est = registration_based_on_corr(source_pcd, target_pcd, max_corr)\n",
    "\n",
    "    T_est = torch.from_numpy(T_est.astype(np.float32))\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    T_est = sym_pose(posF, xyz1, baseF, xyz0, 1)\n",
    "    \n",
    "    # alignment\n",
    "    ali_ret_pc = apply_transform(ret_pc, np.linalg.inv(T_est))\n",
    "    ali_cad = ali_ret_pc*r + offset\n",
    "    \n",
    "    visual_pcs += [pc, ali_cad]\n",
    "    colors += [\"RED\", \"GREEN\"]\n",
    "    \n",
    "    objid = ret_path[0].split('/')[-1].split('.')[0]\n",
    "    #print(objid)\n",
    "    \n",
    "    obj_dir = os.path.join(save_dir, objid+\"_\"+str(idx))\n",
    "    if not os.path.exists(obj_dir):\n",
    "        os.mkdir(obj_dir)\n",
    "        \n",
    "    np.save(os.path.join(obj_dir, \"T_est.npy\"), T_est)\n",
    "    \n",
    "    with open(os.path.join(obj_dir, \"scale_trans.txt\"), \"w\") as f:\n",
    "        f.writelines(str(norm_pc_r)+'\\n')\n",
    "        for val in norm_pc_offset:\n",
    "            f.writelines(str(val)+' ')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines(str(r)+'\\n')\n",
    "        for val in offset:\n",
    "            f.writelines(str(val)+' ')\n",
    "        \n",
    "    \n",
    "    \n",
    "for idx, name in enumerate(tables):\n",
    "    pc = np.load(os.path.join(root, name))\n",
    "    offset = pc.mean(0)\n",
    "    pc_center = pc - offset\n",
    "    r = np.linalg.norm(pc_center, 2, 1).max()\n",
    "    pc_center /= r\n",
    "    #Jvisualize([pc_center], [\"RED\"])\n",
    "    \n",
    "    #feature extraction\n",
    "    base_local_feat, base_global_feat, base_coords = feature_extractor_table.process(pc_center)\n",
    "\n",
    "    # retrieval\n",
    "    ret_path = retrieval_module_table.TopN(base_global_feat.detach().cpu().numpy(), 1)\n",
    "    print(ret_path)\n",
    "\n",
    "    ret_raw_pc = load_raw_pc(ret_path[0], 10000)\n",
    "    \n",
    "    norm_pc_offset = ret_raw_pc.mean(0)\n",
    "    ret_pc = ret_raw_pc - norm_pc_offset\n",
    "    norm_pc_r = np.max(np.linalg.norm(ret_pc,2,1))\n",
    "    ret_pc = ret_pc/norm_pc_r\n",
    "    ret_local_feat, ret_global_feat, ret_coords = feature_extractor_table.process(ret_pc)\n",
    "    \n",
    "    #pose estimation\n",
    "    k_nn=5\n",
    "    max_corr = 0.20\n",
    "\n",
    "    baseF, xyz0, posF, xyz1 = ret_local_feat, ret_coords, base_local_feat, base_coords\n",
    "\n",
    "    \"\"\"\n",
    "    idx_0, idx_1 = find_kcorr(baseF, posF, k=k_nn, subsample_size=-1)\n",
    "\n",
    "    source_pcd = xyz0[idx_0]\n",
    "    target_pcd = xyz1[idx_1]\n",
    "\n",
    "    T_est = registration_based_on_corr(source_pcd, target_pcd, max_corr)\n",
    "\n",
    "    T_est = torch.from_numpy(T_est.astype(np.float32))\n",
    "    \"\"\"\n",
    "    \n",
    "    T_est = sym_pose(posF, xyz1, baseF, xyz0, 2)\n",
    "    \n",
    "    # alignment\n",
    "    ali_ret_pc = apply_transform(ret_pc, np.linalg.inv(T_est))\n",
    "    ali_cad = ali_ret_pc*r + offset\n",
    "    \n",
    "    visual_pcs += [pc, ali_cad]\n",
    "    colors += [\"RED\", \"GREEN\"]\n",
    "\n",
    "    objid = ret_path[0].split('/')[-1].split('.')[0]\n",
    "    #print(objid)\n",
    "    \n",
    "    obj_dir = os.path.join(save_dir, objid+\"_\"+str(idx))\n",
    "    if not os.path.exists(obj_dir):\n",
    "        os.mkdir(obj_dir)\n",
    "        \n",
    "    np.save(os.path.join(obj_dir, \"T_est.npy\"), T_est)\n",
    "    \n",
    "    with open(os.path.join(obj_dir, \"scale_trans.txt\"), \"w\") as f:\n",
    "        f.writelines(str(norm_pc_r)+'\\n')\n",
    "        for val in norm_pc_offset:\n",
    "            f.writelines(str(val)+' ')\n",
    "        f.writelines('\\n')\n",
    "        f.writelines(str(r)+'\\n')\n",
    "        for val in offset:\n",
    "            f.writelines(str(val)+' ')\n",
    "    \n",
    "Jvisualize(visual_pcs, colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sym_pose(baseF, xyz0, posF, xyz1, pos_sym):\n",
    "    #print(pos_sym)\n",
    "\n",
    "    try:\n",
    "        if pos_sym >= 2:\n",
    "            base_mask1, base_mask2, base_mask3, base_mask4 = symmetric_cut4(baseF, xyz0, 4, max_sample=100)\n",
    "\n",
    "            pos_mask1, pos_mask2, pos_mask3, pos_mask4 = symmetric_cut4(posF, xyz1, 4, max_sample=100)\n",
    "\n",
    "            base_masks = [base_mask1, base_mask2, base_mask3, base_mask4]\n",
    "            pos_masks = [pos_mask1, pos_mask2, pos_mask3, pos_mask4]\n",
    "\n",
    "        else:\n",
    "            #print(pos_sym)\n",
    "            base_mask1, base_mask2 = symmetric_cut4(baseF, xyz0, 2, max_sample=100)\n",
    "\n",
    "            pos_mask1, pos_mask2 = symmetric_cut4(posF, xyz1, 2, max_sample=100)\n",
    "\n",
    "            base_masks = [base_mask1, base_mask2]\n",
    "            pos_masks = [pos_mask1, pos_mask2]\n",
    "\n",
    "            #Jvisualize([xyz0[base_mask1], xyz0[base_mask2]], [\"RED\", \"GREEN\"])\n",
    "\n",
    "\n",
    "        chamf_dist_best = 100\n",
    "\n",
    "        for _ in range(len(base_masks)):\n",
    "\n",
    "            pcsA = [xyz0[base_mask] for base_mask in base_masks]\n",
    "            pcsB = [xyz1[pos_mask] for pos_mask in pos_masks]\n",
    "\n",
    "            featsA = [baseF[base_mask] for base_mask in base_masks]\n",
    "            featsB = [posF[pos_mask] for pos_mask in pos_masks]\n",
    "\n",
    "            xyzA_corrs, xyzB_corrs = split_corr(pcsA, pcsB, featsA, featsB, k_nn, subsample_size=-1)\n",
    "\n",
    "            T_est = registration_based_on_corr(xyzA_corrs, xyzB_corrs, max_corr)\n",
    "\n",
    "            T_est = torch.from_numpy(T_est.astype(np.float32))\n",
    "\n",
    "            #t_loss, r_loss = eval_pose(T_est, base_T, pos_T, axis_symmetry=max(base_sym, pos_sym))\n",
    "\n",
    "            chamf_dist = chamfer_gpu_1direction(apply_transform(xyz0, T_est).cuda(), xyz1.cuda())\n",
    "            #avg_dist = np.linalg.norm(apply_transform(xyzA_corrs, T_est) - xyzB_corrs, axis=1).mean()\n",
    "\n",
    "            rot_item = pos_masks.pop(0)\n",
    "            pos_masks.append(rot_item)\n",
    "\n",
    "            #print(chamf_dist)\n",
    "\n",
    "            if chamf_dist_best > chamf_dist:\n",
    "\n",
    "                chamf_dist_best = chamf_dist\n",
    "                T_est_best = T_est\n",
    "\n",
    "\n",
    "        if pos_sym >= 2:\n",
    "            base_masks = [base_mask1, base_mask2, base_mask3, base_mask4]\n",
    "            pos_masks = [pos_mask1, pos_mask4, pos_mask3, pos_mask2]\n",
    "        else:\n",
    "\n",
    "            base_masks = []\n",
    "            pos_masks = []\n",
    "\n",
    "\n",
    "        for _ in range(len(base_masks)):\n",
    "\n",
    "            pcsA = [xyz0[base_mask] for base_mask in base_masks]\n",
    "            pcsB = [xyz1[pos_mask] for pos_mask in pos_masks]\n",
    "\n",
    "            featsA = [baseF[base_mask] for base_mask in base_masks]\n",
    "            featsB = [posF[pos_mask] for pos_mask in pos_masks]\n",
    "\n",
    "            xyzA_corrs, xyzB_corrs = split_corr(pcsA, pcsB, featsA, featsB, k_nn, subsample_size=-1)\n",
    "\n",
    "            T_est = registration_based_on_corr(xyzA_corrs, xyzB_corrs, max_corr)\n",
    "\n",
    "            T_est = torch.from_numpy(T_est.astype(np.float32))\n",
    "\n",
    "            #t_loss, r_loss = eval_pose(T_est, base_T, pos_T, axis_symmetry=max(base_sym, pos_sym))\n",
    "\n",
    "            chamf_dist = chamfer_gpu_1direction(apply_transform(xyz0, T_est).cuda(), xyz1.cuda())\n",
    "            #avg_dist = np.linalg.norm(apply_transform(xyzA_corrs, T_est) - xyzB_corrs, axis=1).mean()\n",
    "\n",
    "            rot_item = pos_masks.pop(0)\n",
    "            pos_masks.append(rot_item)\n",
    "\n",
    "            #print(chamf_dist)\n",
    "\n",
    "            if chamf_dist_best > chamf_dist:\n",
    "\n",
    "                chamf_dist_best = chamf_dist\n",
    "                T_est_best = T_est\n",
    "\n",
    "        idx_0, idx_1 = find_kcorr(baseF, posF, k=k_nn, subsample_size=-1)\n",
    "\n",
    "        source_pcd = xyz0[idx_0]\n",
    "        target_pcd = xyz1[idx_1]\n",
    "\n",
    "        T_est = registration_based_on_corr(source_pcd, target_pcd, max_corr)\n",
    "\n",
    "        T_est = torch.from_numpy(T_est.astype(np.float32))\n",
    "        chamf_dist = chamfer_gpu_1direction(apply_transform(xyz0, T_est).cuda(), xyz1.cuda())\n",
    "        #avg_dist = np.linalg.norm(apply_transform(xyzA_corrs, T_est) - xyzB_corrs, axis=1).mean()  \n",
    "\n",
    "        #print(\"ransac:\")\n",
    "        #print(chamf_dist)\n",
    "\n",
    "        if chamf_dist_best > chamf_dist:\n",
    "\n",
    "            chamf_dist_best = chamf_dist\n",
    "            T_est_best = T_est\n",
    "            print(\"use ransac\")\n",
    "\n",
    "    \n",
    "    except:\n",
    "    \n",
    "        idx_0, idx_1 = find_kcorr(baseF, posF, k=k_nn, subsample_size=-1)\n",
    "\n",
    "        source_pcd = xyz0[idx_0]\n",
    "        target_pcd = xyz1[idx_1]\n",
    "\n",
    "        T_est = registration_based_on_corr(source_pcd, target_pcd, max_corr)\n",
    "\n",
    "        T_est = torch.from_numpy(T_est.astype(np.float32))\n",
    "\n",
    "    return T_est\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2622003767b848eab53d8284c0d0ab16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "JVisualizer with 1 geometries"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Jvisualize([pc],[\"RED\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scene0314_00\n",
    "scene0690_00\n",
    "scene0474_00\n",
    "scene0355_00\n",
    "scene0653_00"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
